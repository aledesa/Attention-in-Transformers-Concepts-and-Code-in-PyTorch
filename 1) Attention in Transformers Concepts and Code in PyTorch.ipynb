{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention in Transformers Concepts and Code in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructor: Josh Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture was first introduced in the 2017 paper [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) for machine translation tasks. The idea was like to input an English sentence and have the network output a German sentence. The same architecture tends to be great at inputting a **prompt** and outputting a **response** to that prompt, like a a question and the answer to that question. So it started the rise of **large language models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Ideas Behind Transformers and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are based on 3 building blocks:\n",
    "\n",
    "1) **Word Embedding**: converts tokens (words, part of words, symbols, etc..) into numbers to be fed into a NN.\n",
    "\n",
    "2) **Positional Encoding**: helps keep track of word order.\n",
    "\n",
    "3) **Attention**: helps establishing relations among words. In *Self-Attention* for each word it calculates the similarity for every word in the sentence. Then the similarities are used to determine how the Transformer encodes each word. In the example below, in the sentences about \"pizza\" the word \"it\" was commonly more associated with \"pizza\" than \"oven\", then the similary score for \"pizza\" will have a larger impact on how the word \"it\" is encoded by the Transformer.\n",
    "\n",
    "<img src=\"images/sa_example.jpg\" width=\"400px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Matrix Math for Calculating Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Self-Attention, the equation to calculate the Attention depends on the 3 inputs Query, Key and Value:\n",
    "\n",
    "$$Attention(Q,K,V) = SoftMax\\bigl(\\frac{QK^T}{\\sqrt{d_k}} \\bigr)V$$\n",
    "\n",
    "The names come from database terminology where the Query is the input used to match a Key, given their similarity, and get the associated Value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how Attention works consider the sentence\n",
    "\n",
    "$$\\text{write a poem}$$\n",
    "\n",
    "And imagin that word embedding converts each of the 3 token in a 2-dimensional vector (typically it would have dimension 512 or more), and stack them one on top of the other:\n",
    "\n",
    "<img src=\"images/encoded_input.jpg\" width=\"200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get the values of Q, K and V we then multiply the encoded values for 3 squared matrices of weights to get the matrices Q, K and V with the same dimension of the encoded values:\n",
    "\n",
    "<img src=\"images/QKV.jpg\" width=\"400px\" />\n",
    "\n",
    "The transpose symbols is because PyTorch prints out the weights in a way that requires them to be transposed before we can get the math correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the first step is to multiply matrix Q by the transpose of K:\n",
    "\n",
    "<img src=\"images/QK.jpg\" width=\"600px\" />\n",
    "\n",
    "Where each entry of the resulting matrix is the *unscaled Dot Product* of all combinations of Queries and Keys for each word. The Dot Product can be used as an unscaled measure of similarity between two things, and it's closely related to the *Cosine Similarity*, with the difference that the latter is scaled to be between $-1$ and $1$.\n",
    "\n",
    "The second step is to scale the Dot Product by $\\sqrt{d_k}$, the square of the dimension of the matrix of Keys, in the example 2. This way we obtain a *scaled Dot Product* of similarities. Note that scaling by the number of values per token doesn't scale the Dot Product in any systematic ways, but the authors claimed it improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to take the soft-max of each row in the matrix of the scaled Dot Product similarities, where the soft-max function is $\\sigma:\\mathbb{R}^K \\rightarrow (0,1)^K$ where $K>1$, takes a vector $z = (z_1, ..., z_K) \\in \\mathbb{R}^K$ and computes each component of vector $\\sigma(z) \\in (0,1)^K$ with\n",
    "\n",
    "$$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "<img src=\"images/softmax.jpg\" width=\"600px\" />\n",
    "\n",
    "Where the sum of each row is $1$, and stack them one on top of the other. This way we can think of these values as a summary of the relationships among tokens. For example the word \"write\" is 36% similar to itself, 40% similar to \"a\" and 24% similar to \"poem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to multiply the soft-max percentages by the Values in matrix V. In this way the first Self-Attention of the word \"write\" is given by calculating 36% of the first Value for the word \"write\", and add it to 40% of the first Value for \"a\", and then add 24% of the first Value for \"poem\", that is $0.60 * 36\\% - 0.35 * 40\\% + 3.86 * 24\\% = 1$. In other words, the percentages that come out of the soft-max function tell us how much influence each word should have on the final encoding for any given word:\n",
    "\n",
    "<img src=\"images/softmax_V.jpg\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary the equation for calculating the Self-Attention is\n",
    "\n",
    "1) calculating the scaled Dot Product similarity among all the words\n",
    "\n",
    "2) converting those scaled similarities into percentages with the soft-max function\n",
    "\n",
    "3) using those percentages to scale the Values to become the Self-Attention scores for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention vs Masked Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple case of creating word embeddings. The training data is made by 2 sentences \"My pizza is great!\" and \"My pizza is awesome\". Each of the 5 *different* words is encoded as a $1$ and passed throught some random weights and some activation functions that, put together will give a probability to the next word. The number of activation functions will be the dimension of the embedding. After some training, the weights will be the word embeddings, and similar words will have closer distance, for example the words \"great\" and \"awesome\":\n",
    "\n",
    "<img src=\"images/embeddings.jpg\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more context one can train word embeddings not only by using one word to predict the next one, but by using many preceeding words at the same time. However, **order also matters**, hence the idea of Positional Encoding: created to take into account word order when creating embeddings and then followed by an Attention layer, that helps establish relationships among words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv37tf",
   "language": "python",
   "name": "myenv37tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
